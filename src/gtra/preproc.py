import numpy as np
import pandas as pd

from collections import Counter, defaultdict

from scipy.stats import mannwhitneyu
from statsmodels.stats.multitest import multipletests


def filter_genes(adata, zero_threshold=.95):
    X = adata.X
    if not isinstance(X, np.ndarray):
        X = X.tocsr()
    n_cells = X.shape[0]
    # nonzero count per gene
    nonzero_counts = np.array((X > 0).sum(axis=0)).ravel()
    # zero ratio
    zero_ratio = 1 - (nonzero_counts / n_cells)
    # keep genes below threshold
    keep_mask = zero_ratio <= zero_threshold
    return adata.var_names[keep_mask].tolist()

# Merge cell type information of total time points
def concat_meta(obj):
    label = obj.params.cell_type_label
    
    # Collect obs subsets (avoid repeated concat inside loop)
    dfs = [obj.tp_data_dict[tp].obs[[label]] 
           for tp in range(obj.tp_data_num)]
    
    # Single concat (fast & stable)
    return pd.concat(dfs, axis=0)
        
# Update index
def update_idx(t1,t2,t1_s,t1_g,t2_s,t2_g):
    if t2_s == len(t2) - 1:
        if t2_g == len(t2[t2_s]) - 1:
            t2_s, t2_g = 0, 0
            t1_g += 1
            if t1_g == len(t1[t1_s]):
                t1_s += 1
                t1_g = 0
        else:
            t2_g += 1
    else:
        if t2_g < len(t2[t2_s]) - 1:
            t2_g += 1
        else:
            t2_s += 1
            t2_g = 0

    return t1_s, t1_g, t2_s, t2_g

# Get unique cell type list
def get_unique_celltype(obj, tp, s):
    label = obj.cell_label_index[tp][s]
    name = obj.params.cell_type_label
    cts = dict(Counter(obj.cell_type_info.loc[label,name].values.tolist()))
    sorted_cts = sorted(cts.items(), key=lambda x:x[1], reverse=True)
    return sorted_cts[0][0]


# Save statistical result's info
def save_stat_res(obj):
    est_edges, score_edges = {}, {}
    for _, v in obj.node_info.iterrows():
        # Get node name
        tok_s = v[0].split('_')
        tok_t = v[1].split('_')
        
        # Get label info
        t1, ct1 = int(tok_s[0][1:]), int(tok_s[1])
        ct2 = int(tok_t[1])
        
        # Save edge info
        edge_name = f'{ct1}_{ct2}'
        
        # Initialization
        if est_edges.get(t1) is None:
            est_edges[t1] = {}
        if score_edges.get(t1) is None:
            score_edges[t1] = {}
        if est_edges[t1].get(edge_name) is None:
            est_edges[t1][edge_name] = 1
        if score_edges[t1].get(edge_name) is None:
            score_edges[t1][edge_name] = v.values[-1]
            
    return est_edges, score_edges


# Get gene cluster's info
def get_gcinfo(obj):
    gene_list = obj.genes
    tp_gcinfo = {}
    for tp in range(obj.tp_data_num):
        n_celltypes = obj.cell_optimal_k[tp]
        ct_gclusters = []
        for clabel in range(n_celltypes):
            n_gc = obj.gene_label_info[tp][clabel]
            gene_to_cluster = {gene: gc for gc, gs in enumerate(n_gc) for gene in gs}
            gclusters = [gene_to_cluster.get(g, 0) for g in gene_list]
            ct_gclusters.append(gclusters)
        tp_gcinfo[tp] = ct_gclusters
    return tp_gcinfo

# Create consensus matrix for gene clustering results
def get_ccmatrix(res):
    ccmatrix = defaultdict(dict)
    N = len(res)
    
    for time in res[0][1].keys():
        cts = len(res[0][1][time])
        for ct in range(cts):
            labels = res[0][1][time][ct]
            n_genes = len(labels)
            mat = np.zeros((n_genes, n_genes))
            
            for run in res:
                labels = np.array(run[1][time][ct])
                same_clusters = (labels[:,None] == labels[None,:]).astype(int)
                mat += same_clusters
            
            mat /= N
            ccmatrix[time][ct] = mat
    return ccmatrix

# Combining parallel processing results
def get_scoreinfo(res):
    """
    Merge parallel processing results from GTra into unified dictionaries.

    This function aggregates outputs generated by parallel workers in GTra.
    Each element in `res` is assumed to have the structure:

        dat[0][0] : dict[str, dict[str, value]]  # merged_dat source
        dat[0][1] : dict[str, dict[str, value]]  # merged_rank source.
    """
    merged_dat = defaultdict(lambda: defaultdict(list))
    merged_score = defaultdict(lambda: defaultdict(list))
    
    for dat in res:
        for key, subdict in dat[0][0].items():
            for subkey, value in subdict.items():
                merged_dat[key][subkey].extend([value])
                
        for key, subdict in dat[0][1].items():
            for subkey, value in subdict.items():
                merged_score[key][subkey].extend([value])
    
    merged_dat = {key: dict(subdict) for key, subdict in merged_dat.items()}
    merged_score = {key: dict(subdict) for key, subdict in merged_score.items()}
    
    return merged_dat, merged_score

def cal_pvals(dist_df):
    """
    Mann-Whitney U-test per (Interval, source)
    - Compare source target rank distribution vs other
    - Apply benjamin-Hochberg FDR correction
    """
    
    results = []
    # For each interval
    for it, df in dist_df.groupby('Interval'):
        # Source cell types
        for ct, cdf in df.groupby('source'):
            raw_pvals, targets = [], []
            
            x = cdf.sort_values('score')
            for tar in np.unique(x['target'].values):
                tar_vals = cdf[cdf['target']==tar]['score'].values
                rest_vals = cdf[cdf['target']!=tar]['score'].values
                
                if len(tar_vals) > 0 and len(rest_vals) >0:
                    _, pval = mannwhitneyu(tar_vals, rest_vals, alternative='greater')
                else:
                    # print(f'[Skip] Empty group')
                    pval = np.nan
                
                raw_pvals.append(pval)
                targets.append(tar)
            
            # ====== FDR correction ====== #
            valid_mask = ~pd.isna(raw_pvals)
            valid_vals = np.array(raw_pvals)[valid_mask]
            
            if np.sum(valid_mask) > 0:
                adj = multipletests(valid_vals, alpha=0.5, method="fdr_bh")[1]
            else:
                adj = np.array([])
            
            adj_pvals = []
            idx_valid = 0
            for valid in valid_mask:
                if valid:
                    adj_pvals.append(adj[idx_valid])
                    idx_valid+=1
                else:
                    adj_pvals.append(np.nan)
            
            for tar, raw_p, adj_p in zip(targets, raw_pvals, adj_pvals):
                results.append([it, ct, tar, raw_p, adj_p])
    pval_df = pd.DataFrame(results, columns=[
        'Interval','source','target','p-value','adj_p-value'
    ])
    return pval_df
    